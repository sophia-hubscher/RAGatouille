#Dataset Interface:
import math
import statistics
import pandas as pd
import openai
from metrics_config import accuracy_prompt, relevance_prompt, groundedness_prompt

class Metrics:

    #you only need to worry about init, eval_row, and macro_metrics. Ignore the rest.

    def __init__(self):
        self.results = []
        self.open_ai = openai.OpenAI()

    def precision(self, retrieved: list, relevant: list) -> float:
        if len(retrieved) == 0: return 0
        if len(relevant) == 0: return 0

        true_positive: int = 0
        for i in retrieved:
            if i in relevant:
                true_positive += 1

        return true_positive / len(retrieved)

    def precision_at_k(self, retrieved: list, relevant: list, k: int) -> float:
        assert k > 0, "k must be greater than 0"
        return self.precision(retrieved[:k], relevant)

    def recall(self, retrieved: list, relevant: list) -> float:
        if len(retrieved) == 0: return 0
        if len(relevant) == 0: return 1

        true_positive: int = 0
        for i in retrieved:
            if i in relevant:
                true_positive += 1

        return true_positive/len(relevant)

    def recall_at_k(self, retrieved: list, relevant: list, k: int) -> float:
        assert k>0, "k must be greater than 0"
        return self.recall(retrieved[:k], relevant)

    def f1(self, retrieved: list, relevant: list) -> float:
        p = self.precision(retrieved, relevant)
        r = self.recall(retrieved, relevant)

        denominator = p + r
        if denominator == 0:
            return 0

        return (2 * p * r) / denominator

    #generation metrics:
    def accuracy(self, generated: str, correct: str) -> float:
        response = self.open_ai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                "role": "system",
                "content": accuracy_prompt
                },
                {
                "role": "user",
                "content": "llm generated answer: " + generated + "\n correct answer: " + correct
                }
            ]
        )
        accuracy = int(response.choices[0].message.content[0])
        return accuracy/5
    def relevance(self, generated: str, ground_truth: list) -> float:
        response = self.open_ai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": relevance_prompt
                },
                {
                    "role": "user",
                    "content": "llm generated answer: " + generated + "\n  ground truth: " + ground_truth
                }
            ]
        )
        relevance = int(response.choices[0].message.content[0])
        return relevance / 5
    def groundedness(self, generated: str, retrieved: list) -> float:
        response = self.open_ai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": groundedness_prompt
                },
                {
                    "role": "user",
                    "content": "llm generated answer: " + generated + "\n   retrieved context: " + "".join(retrieved)
                }
            ]
        )
        groundedness = int(response.choices[0].message.content[0])
        return groundedness / 5

    #running eval:
    def eval_row(
            self,
            question: str, #question from the dataset
            correct_answer: str, #the correct answer from the dataset
            generated_answer: str, #answer generated by the RAG chain
            relevant_docs: list, #relevant document IDs
            retrieved_docs: list, #retrieved document IDs
            retrieved_chunks: list, #retrieved chunks of text
            ground_truth: list[str],  # list of ground truth phrases from the dataset
    ):

        #retrieval metrics:
        p = self.precision(retrieved_docs, relevant_docs)
        p_at_k = [self.precision_at_k(retrieved_docs, relevant_docs, i+1) for i in range(len(retrieved_docs))]

        r = self.recall(retrieved_docs, relevant_docs)
        r_at_k = [self.recall_at_k(retrieved_docs, relevant_docs, i+1) for i in range(len(retrieved_docs))]

        f1 = self.f1(retrieved_docs, relevant_docs)
        #generation metrics:

        a = self.accuracy(generated_answer, correct_answer)
        rel = self.relevance(generated_answer, ground_truth)
        g = self.groundedness(generated_answer, retrieved_chunks)

        #return:
        result = {
            "precision": p,
            "precisions_at_k": p_at_k,
            "recall": r,
            "recalls_at_k": r_at_k,
            "f1": f1,
            "accuracy": a,
            "relevance": rel,
            "groundedness": g
        }
        self.results.append(result)
        return result

    def macro_metrics(self):
        #Run ONCE after running eval_row on all rows
        mean_recall = statistics.mean([result["recall"] for result in self.results])
        mean_accuracy = statistics.mean([result["accuracy"] for result in self.results])
        mean_relevance = statistics.mean([result["relevance"] for result in self.results])
        mean_groundedness = statistics.mean([result["groundedness"] for result in self.results])
        mean_recall_at_1 = statistics.mean([result["recall_at_k"][0] for result in self.results])

        return {
            "mean_recall": mean_recall,
            "mean_accuracy": mean_accuracy,
            "mean_relevance": mean_relevance,
            "mean_groundedness": mean_groundedness,
            "mean_recall_at_1": mean_recall_at_1
        }